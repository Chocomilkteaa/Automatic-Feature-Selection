{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearnex import patch_sklearn\n",
    "\n",
    "n_cpu = os.cpu_count()-3\n",
    "\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processNSLKDD():\n",
    "    arff_train = arff.loadarff('./Data/nsl-kdd/KDDTrain+.arff')\n",
    "    arff_test = arff.loadarff('./Data/nsl-kdd/KDDTest+.arff')\n",
    "    train_data = pd.DataFrame(arff_train[0])\n",
    "    test_data = pd.DataFrame(arff_test[0])\n",
    "    \n",
    "    for f in test_data.select_dtypes(include='O').columns:\n",
    "        train_data[f] = train_data[f].str.decode(encoding='utf-8')\n",
    "        test_data[f] = test_data[f].str.decode(encoding='utf-8')\n",
    "\n",
    "    for f in ['land', 'logged_in', 'is_host_login', 'is_guest_login']:\n",
    "        train_data[f] = train_data[f].map({'0': 0, '1': 1})\n",
    "        test_data[f] = test_data[f].map({'0': 0, '1': 1})\n",
    "\n",
    "    X_train = train_data.drop(['class', 'num_outbound_cmds'], axis=1).select_dtypes(include='number')\n",
    "\n",
    "    X_max, X_min = X_train.max(axis=0), X_train.min(axis=0)\n",
    "    X_train = (X_train - X_min) / (X_max - X_min)\n",
    "\n",
    "    Y_train = train_data['class'].map({'normal': 1, 'anomaly': 0})\n",
    "\n",
    "    X_test = test_data.drop(['class', 'num_outbound_cmds'], axis=1).select_dtypes(include='number')\n",
    "\n",
    "    X_test = (X_test - X_min) / (X_max - X_min)\n",
    "    X_test.clip(0, 1)\n",
    "\n",
    "    Y_test = test_data['class'].map({'normal': 1, 'anomaly': 0})\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = processNSLKDD()\n",
    "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_result = pd.read_pickle('./Result/columns_result.pkl')\n",
    "columns_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validateAndTest(model, X_train, Y_train, X_test, Y_test):\n",
    "    cv = StratifiedKFold(shuffle=True, random_state=0)\n",
    "    cv_score = cross_val_score(model, X_train, Y_train, scoring='f1', cv=cv, n_jobs=n_cpu).mean()\n",
    "\n",
    "    model.fit(X_train.values, Y_train.values)\n",
    "    predictions = model.predict(X_test.values)\n",
    "    test_score = f1_score(Y_test.values, predictions)\n",
    "\n",
    "    return cv_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEvaluateResult(X_train, Y_train, X_test, Y_test):\n",
    "    scores = pd.DataFrame()\n",
    "    \n",
    "    for model, model_name in zip([LogisticRegression(C=100, solver='liblinear', random_state=0),\n",
    "                                  GradientBoostingClassifier(n_estimators=200, random_state=0)], ['LR', 'GB']):\n",
    "        for method in ['UFS', 'SBS', 'RFE', 'Importance', 'Greedy']:\n",
    "            columns_to_select = []\n",
    "            cv_scores, test_scores = [], []\n",
    "            for _, row in columns_result[method].items():\n",
    "                columns_to_select += [row]\n",
    "\n",
    "                cv_score, test_score = validateAndTest(model, X_train[columns_to_select], Y_train, X_test[columns_to_select], Y_test)\n",
    "                cv_scores.append(cv_score)\n",
    "                test_scores.append(test_score)\n",
    "\n",
    "            scores[f'cv_{method}_{model_name}'] = cv_scores\n",
    "            scores[f'test_{method}_{model_name}'] = test_scores\n",
    "\n",
    "        for method in ['Union', 'Intersection', 'Quorum']:\n",
    "            cv_scores, test_scores = [], []\n",
    "            prev = None\n",
    "            for _, columns_to_select in columns_result[method].items():\n",
    "                if len(columns_to_select) > 1:\n",
    "                    if columns_to_select != prev:\n",
    "                        cv_score, test_score = validateAndTest(model, X_train[columns_to_select], Y_train, X_test[columns_to_select], Y_test)\n",
    "                        cv_scores.append(cv_score)\n",
    "                        test_scores.append(test_score)\n",
    "                    else:\n",
    "                        cv_scores.append(cv_scores[-1])\n",
    "                        test_scores.append(test_scores[-1])\n",
    "                else:\n",
    "                    cv_scores.append(0)\n",
    "                    test_scores.append(0)\n",
    "                \n",
    "                prev = columns_to_select\n",
    "            \n",
    "            scores[f'cv_{method}_{model_name}'] = cv_scores\n",
    "            scores[f'test_{method}_{model_name}'] = test_scores\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = getEvaluateResult(X_train.iloc[:100], Y_train.iloc[:100], X_test.iloc[:100], Y_test.iloc[:100])\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.to_pickle('./Result/scores_result.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopping Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MaxDelta(score):\n",
    "    max_delta = 0\n",
    "    index = len(score) - 1\n",
    "    for i in range(len(score)-1):\n",
    "        delta = score[i+1] - score[i]\n",
    "        if delta > max_delta:\n",
    "            max_delta = delta\n",
    "            index = i\n",
    "\n",
    "    return index\n",
    "\n",
    "def MinPerfReq(score, tolerence):\n",
    "    score_req = score[-1] * (1 - tolerence)\n",
    "    for i in range(len(score)-1, 0, -1):\n",
    "        if score[i] < score_req:\n",
    "            return i+1\n",
    "\n",
    "    return len(score) - 1\n",
    "\n",
    "def MaxScore(score, size, factor):\n",
    "    best_performance = 0\n",
    "    index = len(score) - 1\n",
    "    for i in range(len(score)):\n",
    "        adj_score = score[i] - (factor * size[i])\n",
    "        if adj_score > best_performance:\n",
    "            best_performance = adj_score\n",
    "            index = i\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerence = 0.03\n",
    "factor = 0.03\n",
    "\n",
    "stopping_points = {}\n",
    "\n",
    "for model_name in ['LR', 'GB']:\n",
    "    for method in ['UFS', 'SBS', 'RFE', 'Importance', 'Union', 'Intersection', 'Quorum', 'Greedy']:\n",
    "        cv_scores = scores[f'cv_{method}_{model_name}'].to_list()\n",
    "        test_scores = scores[f'test_{method}_{model_name}'].to_list()\n",
    "\n",
    "        if method in ['Union', 'Intersection', 'Quorum']:\n",
    "            subset_size = [len(i) for i in columns_result[method]]\n",
    "        else:\n",
    "            subset_size = [i+1 for i in range(scores.shape[0])]\n",
    "\n",
    "        stop1 = MaxDelta(cv_scores)\n",
    "        stop2 = MinPerfReq(cv_scores, tolerence)\n",
    "        stop3 = MaxScore(cv_scores, subset_size, factor)\n",
    "\n",
    "        for stop_name, index in zip(['MaxDelta', 'MinPerfReq', 'MaxScore'], [stop1, stop2, stop3]):\n",
    "            stopping_points[f'{stop_name}_size_{method}_{model_name}'] = subset_size[index]\n",
    "            stopping_points[f'{stop_name}_cv_score_{method}_{model_name}'] = cv_scores[index]\n",
    "            stopping_points[f'{stop_name}_test_score_{method}_{model_name}'] = test_scores[index]\n",
    "\n",
    "stopping_points = pd.Series(stopping_points)\n",
    "stopping_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopping_points.to_pickle('./Result/stopping_points.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
